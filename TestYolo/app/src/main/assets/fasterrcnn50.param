7767517
550 573
pnnx.Input               pnnx_input_0             0 1 0 #0=(1,3,800,800)f32
pnnx.Expression          pnnx_expr_1829           0 1 1 expr=False
pnnx.Expression          pnnx_expr_1827           0 1 2 expr=6
torch.unbind             Tensor.select_599        1 1 0 3 dim=0 #0=(1,3,800,800)f32 #3=(3,800,800)f32
pnnx.Attribute           pnnx_fold_66             0 1 4 @data=(3,1,1)f32 #4=(3,1,1)f32
pnnx.Attribute           pnnx_fold_70             0 1 5 @data=(3,1,1)f32 #5=(3,1,1)f32
pnnx.Expression          pnnx_expr_1804           3 1 3 4 5 6 expr=div(sub(@0,@1),@2) #3=(3,800,800)f32 #4=(3,1,1)f32 #5=(3,1,1)f32 #6=(3,800,800)f32
torch.unsqueeze          torch.unsqueeze_545      1 1 6 7 dim=0 $input=6 #6=(3,800,800)f32 #7=(1,3,800,800)f32
torch.unbind             Tensor.select_608        1 1 7 8 dim=0 #7=(1,3,800,800)f32 #8=(3,800,800)f32
torch.stack              torch.stack_231          1 1 8 9 dim=0 #8=(3,800,800)f32 #9=(1,3,800,800)f32
nn.Conv2d                m.backbone.body.conv1    1 1 9 10 bias=False dilation=(1,1) groups=1 in_channels=3 kernel_size=(7,7) out_channels=64 padding=(3,3) padding_mode=zeros stride=(2,2) @weight=(64,3,7,7)f32 #9=(1,3,800,800)f32 #10=(1,64,400,400)f32
pnnx.Attribute           pnnx_fold_scale.8        0 1 11 @data=(1,64,1,1)f32 #11=(1,64,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.8        0 1 12 @data=(1,64,1,1)f32 #12=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_1675           3 1 10 11 12 13 expr=add(mul(@0,@1),@2) #10=(1,64,400,400)f32 #11=(1,64,1,1)f32 #12=(1,64,1,1)f32 #13=(1,64,400,400)f32
nn.ReLU                  m.backbone.body.relu     1 1 13 14 #13=(1,64,400,400)f32 #14=(1,64,400,400)f32
nn.MaxPool2d             m.backbone.body.maxpool  1 1 14 15 ceil_mode=False dilation=(1,1) kernel_size=(3,3) padding=(1,1) return_indices=False stride=(2,2) #14=(1,64,400,400)f32 #15=(1,64,200,200)f32
nn.Conv2d                m.backbone.body.layer1.0.conv1 1 1 15 16 bias=False dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=64 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(64,64,1,1)f32 #15=(1,64,200,200)f32 #16=(1,64,200,200)f32
pnnx.Attribute           pnnx_fold_scale.10       0 1 17 @data=(1,64,1,1)f32 #17=(1,64,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.10       0 1 18 @data=(1,64,1,1)f32 #18=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_1662           3 1 16 17 18 19 expr=add(mul(@0,@1),@2) #16=(1,64,200,200)f32 #17=(1,64,1,1)f32 #18=(1,64,1,1)f32 #19=(1,64,200,200)f32
nn.ReLU                  m.backbone.body.layer1.0.relu 1 1 19 20 #19=(1,64,200,200)f32 #20=(1,64,200,200)f32
nn.Conv2d                m.backbone.body.layer1.0.conv2 1 1 20 21 bias=False dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(64,64,3,3)f32 #20=(1,64,200,200)f32 #21=(1,64,200,200)f32
pnnx.Attribute           pnnx_fold_scale.12       0 1 22 @data=(1,64,1,1)f32 #22=(1,64,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.12       0 1 23 @data=(1,64,1,1)f32 #23=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_1650           3 1 21 22 23 24 expr=add(mul(@0,@1),@2) #21=(1,64,200,200)f32 #22=(1,64,1,1)f32 #23=(1,64,1,1)f32 #24=(1,64,200,200)f32
nn.ReLU                  pnnx_unique_9            1 1 24 25 #24=(1,64,200,200)f32 #25=(1,64,200,200)f32
nn.Conv2d                m.backbone.body.layer1.0.conv3 1 1 25 26 bias=False dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,64,1,1)f32 #25=(1,64,200,200)f32 #26=(1,256,200,200)f32
pnnx.Attribute           pnnx_fold_scale.14       0 1 27 @data=(1,256,1,1)f32 #27=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.14       0 1 28 @data=(1,256,1,1)f32 #28=(1,256,1,1)f32
nn.Conv2d                m.backbone.body.layer1.0.downsample.0 1 1 15 29 bias=False dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,64,1,1)f32 #15=(1,64,200,200)f32 #29=(1,256,200,200)f32
pnnx.Attribute           pnnx_fold_scale.16       0 1 30 @data=(1,256,1,1)f32 #30=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.16       0 1 31 @data=(1,256,1,1)f32 #31=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1625           6 1 26 27 28 29 30 31 32 expr=add(add(mul(@0,@1),@2),add(mul(@3,@4),@5)) #26=(1,256,200,200)f32 #27=(1,256,1,1)f32 #28=(1,256,1,1)f32 #29=(1,256,200,200)f32 #30=(1,256,1,1)f32 #31=(1,256,1,1)f32 #32=(1,256,200,200)f32
nn.ReLU                  pnnx_unique_16           1 1 32 33 #32=(1,256,200,200)f32 #33=(1,256,200,200)f32
nn.Conv2d                m.backbone.body.layer1.1.conv1 1 1 33 34 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=64 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(64,256,1,1)f32 #33=(1,256,200,200)f32 #34=(1,64,200,200)f32
pnnx.Attribute           pnnx_fold_scale.18       0 1 35 @data=(1,64,1,1)f32 #35=(1,64,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.18       0 1 36 @data=(1,64,1,1)f32 #36=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_1612           3 1 34 35 36 37 expr=add(mul(@0,@1),@2) #34=(1,64,200,200)f32 #35=(1,64,1,1)f32 #36=(1,64,1,1)f32 #37=(1,64,200,200)f32
nn.ReLU                  m.backbone.body.layer1.1.relu 1 1 37 38 #37=(1,64,200,200)f32 #38=(1,64,200,200)f32
nn.Conv2d                m.backbone.body.layer1.1.conv2 1 1 38 39 bias=False dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(64,64,3,3)f32 #38=(1,64,200,200)f32 #39=(1,64,200,200)f32
pnnx.Attribute           pnnx_fold_scale.20       0 1 40 @data=(1,64,1,1)f32 #40=(1,64,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.20       0 1 41 @data=(1,64,1,1)f32 #41=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_1600           3 1 39 40 41 42 expr=add(mul(@0,@1),@2) #39=(1,64,200,200)f32 #40=(1,64,1,1)f32 #41=(1,64,1,1)f32 #42=(1,64,200,200)f32
nn.ReLU                  pnnx_unique_23           1 1 42 43 #42=(1,64,200,200)f32 #43=(1,64,200,200)f32
nn.Conv2d                m.backbone.body.layer1.1.conv3 1 1 43 44 bias=False dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,64,1,1)f32 #43=(1,64,200,200)f32 #44=(1,256,200,200)f32
pnnx.Attribute           pnnx_fold_scale.22       0 1 45 @data=(1,256,1,1)f32 #45=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.22       0 1 46 @data=(1,256,1,1)f32 #46=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1587           4 1 44 45 46 33 47 expr=add(add(mul(@0,@1),@2),@3) #44=(1,256,200,200)f32 #45=(1,256,1,1)f32 #46=(1,256,1,1)f32 #33=(1,256,200,200)f32 #47=(1,256,200,200)f32
nn.ReLU                  pnnx_unique_27           1 1 47 48 #47=(1,256,200,200)f32 #48=(1,256,200,200)f32
nn.Conv2d                m.backbone.body.layer1.2.conv1 1 1 48 49 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=64 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(64,256,1,1)f32 #48=(1,256,200,200)f32 #49=(1,64,200,200)f32
pnnx.Attribute           pnnx_fold_scale.24       0 1 50 @data=(1,64,1,1)f32 #50=(1,64,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.24       0 1 51 @data=(1,64,1,1)f32 #51=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_1574           3 1 49 50 51 52 expr=add(mul(@0,@1),@2) #49=(1,64,200,200)f32 #50=(1,64,1,1)f32 #51=(1,64,1,1)f32 #52=(1,64,200,200)f32
nn.ReLU                  m.backbone.body.layer1.2.relu 1 1 52 53 #52=(1,64,200,200)f32 #53=(1,64,200,200)f32
nn.Conv2d                m.backbone.body.layer1.2.conv2 1 1 53 54 bias=False dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(64,64,3,3)f32 #53=(1,64,200,200)f32 #54=(1,64,200,200)f32
pnnx.Attribute           pnnx_fold_scale.26       0 1 55 @data=(1,64,1,1)f32 #55=(1,64,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.26       0 1 56 @data=(1,64,1,1)f32 #56=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_1562           3 1 54 55 56 57 expr=add(mul(@0,@1),@2) #54=(1,64,200,200)f32 #55=(1,64,1,1)f32 #56=(1,64,1,1)f32 #57=(1,64,200,200)f32
nn.ReLU                  pnnx_unique_34           1 1 57 58 #57=(1,64,200,200)f32 #58=(1,64,200,200)f32
nn.Conv2d                m.backbone.body.layer1.2.conv3 1 1 58 59 bias=False dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,64,1,1)f32 #58=(1,64,200,200)f32 #59=(1,256,200,200)f32
pnnx.Attribute           pnnx_fold_scale.28       0 1 60 @data=(1,256,1,1)f32 #60=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.28       0 1 61 @data=(1,256,1,1)f32 #61=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1549           4 1 59 60 61 48 62 expr=add(add(mul(@0,@1),@2),@3) #59=(1,256,200,200)f32 #60=(1,256,1,1)f32 #61=(1,256,1,1)f32 #48=(1,256,200,200)f32 #62=(1,256,200,200)f32
nn.ReLU                  pnnx_unique_38           1 1 62 63 #62=(1,256,200,200)f32 #63=(1,256,200,200)f32
nn.Conv2d                m.backbone.body.layer2.0.conv1 1 1 63 64 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(128,256,1,1)f32 #63=(1,256,200,200)f32 #64=(1,128,200,200)f32
pnnx.Attribute           pnnx_fold_scale.30       0 1 65 @data=(1,128,1,1)f32 #65=(1,128,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.30       0 1 66 @data=(1,128,1,1)f32 #66=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_1536           3 1 64 65 66 67 expr=add(mul(@0,@1),@2) #64=(1,128,200,200)f32 #65=(1,128,1,1)f32 #66=(1,128,1,1)f32 #67=(1,128,200,200)f32
nn.ReLU                  m.backbone.body.layer2.0.relu 1 1 67 68 #67=(1,128,200,200)f32 #68=(1,128,200,200)f32
nn.Conv2d                m.backbone.body.layer2.0.conv2 1 1 68 69 bias=False dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(2,2) @weight=(128,128,3,3)f32 #68=(1,128,200,200)f32 #69=(1,128,100,100)f32
pnnx.Attribute           pnnx_fold_scale.32       0 1 70 @data=(1,128,1,1)f32 #70=(1,128,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.32       0 1 71 @data=(1,128,1,1)f32 #71=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_1524           3 1 69 70 71 72 expr=add(mul(@0,@1),@2) #69=(1,128,100,100)f32 #70=(1,128,1,1)f32 #71=(1,128,1,1)f32 #72=(1,128,100,100)f32
nn.ReLU                  pnnx_unique_45           1 1 72 73 #72=(1,128,100,100)f32 #73=(1,128,100,100)f32
nn.Conv2d                m.backbone.body.layer2.0.conv3 1 1 73 74 bias=False dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(512,128,1,1)f32 #73=(1,128,100,100)f32 #74=(1,512,100,100)f32
pnnx.Attribute           pnnx_fold_scale.34       0 1 75 @data=(1,512,1,1)f32 #75=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.34       0 1 76 @data=(1,512,1,1)f32 #76=(1,512,1,1)f32
nn.Conv2d                m.backbone.body.layer2.0.downsample.0 1 1 63 77 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(2,2) @weight=(512,256,1,1)f32 #63=(1,256,200,200)f32 #77=(1,512,100,100)f32
pnnx.Attribute           pnnx_fold_scale.36       0 1 78 @data=(1,512,1,1)f32 #78=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.36       0 1 79 @data=(1,512,1,1)f32 #79=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1499           6 1 74 75 76 77 78 79 80 expr=add(add(mul(@0,@1),@2),add(mul(@3,@4),@5)) #74=(1,512,100,100)f32 #75=(1,512,1,1)f32 #76=(1,512,1,1)f32 #77=(1,512,100,100)f32 #78=(1,512,1,1)f32 #79=(1,512,1,1)f32 #80=(1,512,100,100)f32
nn.ReLU                  pnnx_unique_52           1 1 80 81 #80=(1,512,100,100)f32 #81=(1,512,100,100)f32
nn.Conv2d                m.backbone.body.layer2.1.conv1 1 1 81 82 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(128,512,1,1)f32 #81=(1,512,100,100)f32 #82=(1,128,100,100)f32
pnnx.Attribute           pnnx_fold_scale.38       0 1 83 @data=(1,128,1,1)f32 #83=(1,128,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.38       0 1 84 @data=(1,128,1,1)f32 #84=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_1486           3 1 82 83 84 85 expr=add(mul(@0,@1),@2) #82=(1,128,100,100)f32 #83=(1,128,1,1)f32 #84=(1,128,1,1)f32 #85=(1,128,100,100)f32
nn.ReLU                  m.backbone.body.layer2.1.relu 1 1 85 86 #85=(1,128,100,100)f32 #86=(1,128,100,100)f32
nn.Conv2d                m.backbone.body.layer2.1.conv2 1 1 86 87 bias=False dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(128,128,3,3)f32 #86=(1,128,100,100)f32 #87=(1,128,100,100)f32
pnnx.Attribute           pnnx_fold_scale.40       0 1 88 @data=(1,128,1,1)f32 #88=(1,128,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.40       0 1 89 @data=(1,128,1,1)f32 #89=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_1474           3 1 87 88 89 90 expr=add(mul(@0,@1),@2) #87=(1,128,100,100)f32 #88=(1,128,1,1)f32 #89=(1,128,1,1)f32 #90=(1,128,100,100)f32
nn.ReLU                  pnnx_unique_59           1 1 90 91 #90=(1,128,100,100)f32 #91=(1,128,100,100)f32
nn.Conv2d                m.backbone.body.layer2.1.conv3 1 1 91 92 bias=False dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(512,128,1,1)f32 #91=(1,128,100,100)f32 #92=(1,512,100,100)f32
pnnx.Attribute           pnnx_fold_scale.42       0 1 93 @data=(1,512,1,1)f32 #93=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.42       0 1 94 @data=(1,512,1,1)f32 #94=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1461           4 1 92 93 94 81 95 expr=add(add(mul(@0,@1),@2),@3) #92=(1,512,100,100)f32 #93=(1,512,1,1)f32 #94=(1,512,1,1)f32 #81=(1,512,100,100)f32 #95=(1,512,100,100)f32
nn.ReLU                  pnnx_unique_63           1 1 95 96 #95=(1,512,100,100)f32 #96=(1,512,100,100)f32
nn.Conv2d                m.backbone.body.layer2.2.conv1 1 1 96 97 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(128,512,1,1)f32 #96=(1,512,100,100)f32 #97=(1,128,100,100)f32
pnnx.Attribute           pnnx_fold_scale.44       0 1 98 @data=(1,128,1,1)f32 #98=(1,128,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.44       0 1 99 @data=(1,128,1,1)f32 #99=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_1448           3 1 97 98 99 100 expr=add(mul(@0,@1),@2) #97=(1,128,100,100)f32 #98=(1,128,1,1)f32 #99=(1,128,1,1)f32 #100=(1,128,100,100)f32
nn.ReLU                  m.backbone.body.layer2.2.relu 1 1 100 101 #100=(1,128,100,100)f32 #101=(1,128,100,100)f32
nn.Conv2d                m.backbone.body.layer2.2.conv2 1 1 101 102 bias=False dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(128,128,3,3)f32 #101=(1,128,100,100)f32 #102=(1,128,100,100)f32
pnnx.Attribute           pnnx_fold_scale.46       0 1 103 @data=(1,128,1,1)f32 #103=(1,128,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.46       0 1 104 @data=(1,128,1,1)f32 #104=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_1436           3 1 102 103 104 105 expr=add(mul(@0,@1),@2) #102=(1,128,100,100)f32 #103=(1,128,1,1)f32 #104=(1,128,1,1)f32 #105=(1,128,100,100)f32
nn.ReLU                  pnnx_unique_70           1 1 105 106 #105=(1,128,100,100)f32 #106=(1,128,100,100)f32
nn.Conv2d                m.backbone.body.layer2.2.conv3 1 1 106 107 bias=False dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(512,128,1,1)f32 #106=(1,128,100,100)f32 #107=(1,512,100,100)f32
pnnx.Attribute           pnnx_fold_scale.48       0 1 108 @data=(1,512,1,1)f32 #108=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.48       0 1 109 @data=(1,512,1,1)f32 #109=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1423           4 1 107 108 109 96 110 expr=add(add(mul(@0,@1),@2),@3) #107=(1,512,100,100)f32 #108=(1,512,1,1)f32 #109=(1,512,1,1)f32 #96=(1,512,100,100)f32 #110=(1,512,100,100)f32
nn.ReLU                  pnnx_unique_74           1 1 110 111 #110=(1,512,100,100)f32 #111=(1,512,100,100)f32
nn.Conv2d                m.backbone.body.layer2.3.conv1 1 1 111 112 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(128,512,1,1)f32 #111=(1,512,100,100)f32 #112=(1,128,100,100)f32
pnnx.Attribute           pnnx_fold_scale.50       0 1 113 @data=(1,128,1,1)f32 #113=(1,128,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.50       0 1 114 @data=(1,128,1,1)f32 #114=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_1410           3 1 112 113 114 115 expr=add(mul(@0,@1),@2) #112=(1,128,100,100)f32 #113=(1,128,1,1)f32 #114=(1,128,1,1)f32 #115=(1,128,100,100)f32
nn.ReLU                  m.backbone.body.layer2.3.relu 1 1 115 116 #115=(1,128,100,100)f32 #116=(1,128,100,100)f32
nn.Conv2d                m.backbone.body.layer2.3.conv2 1 1 116 117 bias=False dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(128,128,3,3)f32 #116=(1,128,100,100)f32 #117=(1,128,100,100)f32
pnnx.Attribute           pnnx_fold_scale.52       0 1 118 @data=(1,128,1,1)f32 #118=(1,128,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.52       0 1 119 @data=(1,128,1,1)f32 #119=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_1398           3 1 117 118 119 120 expr=add(mul(@0,@1),@2) #117=(1,128,100,100)f32 #118=(1,128,1,1)f32 #119=(1,128,1,1)f32 #120=(1,128,100,100)f32
nn.ReLU                  pnnx_unique_81           1 1 120 121 #120=(1,128,100,100)f32 #121=(1,128,100,100)f32
nn.Conv2d                m.backbone.body.layer2.3.conv3 1 1 121 122 bias=False dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(512,128,1,1)f32 #121=(1,128,100,100)f32 #122=(1,512,100,100)f32
pnnx.Attribute           pnnx_fold_scale.54       0 1 123 @data=(1,512,1,1)f32 #123=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.54       0 1 124 @data=(1,512,1,1)f32 #124=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1385           4 1 122 123 124 111 125 expr=add(add(mul(@0,@1),@2),@3) #122=(1,512,100,100)f32 #123=(1,512,1,1)f32 #124=(1,512,1,1)f32 #111=(1,512,100,100)f32 #125=(1,512,100,100)f32
nn.ReLU                  pnnx_unique_85           1 1 125 126 #125=(1,512,100,100)f32 #126=(1,512,100,100)f32
nn.Conv2d                m.backbone.body.layer3.0.conv1 1 1 126 127 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,512,1,1)f32 #126=(1,512,100,100)f32 #127=(1,256,100,100)f32
pnnx.Attribute           pnnx_fold_scale.56       0 1 128 @data=(1,256,1,1)f32 #128=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.56       0 1 129 @data=(1,256,1,1)f32 #129=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1372           3 1 127 128 129 130 expr=add(mul(@0,@1),@2) #127=(1,256,100,100)f32 #128=(1,256,1,1)f32 #129=(1,256,1,1)f32 #130=(1,256,100,100)f32
nn.ReLU                  m.backbone.body.layer3.0.relu 1 1 130 131 #130=(1,256,100,100)f32 #131=(1,256,100,100)f32
nn.Conv2d                m.backbone.body.layer3.0.conv2 1 1 131 132 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(2,2) @weight=(256,256,3,3)f32 #131=(1,256,100,100)f32 #132=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.58       0 1 133 @data=(1,256,1,1)f32 #133=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.58       0 1 134 @data=(1,256,1,1)f32 #134=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1360           3 1 132 133 134 135 expr=add(mul(@0,@1),@2) #132=(1,256,50,50)f32 #133=(1,256,1,1)f32 #134=(1,256,1,1)f32 #135=(1,256,50,50)f32
nn.ReLU                  pnnx_unique_92           1 1 135 136 #135=(1,256,50,50)f32 #136=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.0.conv3 1 1 136 137 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=1024 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(1024,256,1,1)f32 #136=(1,256,50,50)f32 #137=(1,1024,50,50)f32
pnnx.Attribute           pnnx_fold_scale.60       0 1 138 @data=(1,1024,1,1)f32 #138=(1,1024,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.60       0 1 139 @data=(1,1024,1,1)f32 #139=(1,1024,1,1)f32
nn.Conv2d                m.backbone.body.layer3.0.downsample.0 1 1 126 140 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=1024 padding=(0,0) padding_mode=zeros stride=(2,2) @weight=(1024,512,1,1)f32 #126=(1,512,100,100)f32 #140=(1,1024,50,50)f32
pnnx.Attribute           pnnx_fold_scale.62       0 1 141 @data=(1,1024,1,1)f32 #141=(1,1024,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.62       0 1 142 @data=(1,1024,1,1)f32 #142=(1,1024,1,1)f32
pnnx.Expression          pnnx_expr_1335           6 1 137 138 139 140 141 142 143 expr=add(add(mul(@0,@1),@2),add(mul(@3,@4),@5)) #137=(1,1024,50,50)f32 #138=(1,1024,1,1)f32 #139=(1,1024,1,1)f32 #140=(1,1024,50,50)f32 #141=(1,1024,1,1)f32 #142=(1,1024,1,1)f32 #143=(1,1024,50,50)f32
nn.ReLU                  pnnx_unique_99           1 1 143 144 #143=(1,1024,50,50)f32 #144=(1,1024,50,50)f32
nn.Conv2d                m.backbone.body.layer3.1.conv1 1 1 144 145 bias=False dilation=(1,1) groups=1 in_channels=1024 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,1024,1,1)f32 #144=(1,1024,50,50)f32 #145=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.64       0 1 146 @data=(1,256,1,1)f32 #146=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.64       0 1 147 @data=(1,256,1,1)f32 #147=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1322           3 1 145 146 147 148 expr=add(mul(@0,@1),@2) #145=(1,256,50,50)f32 #146=(1,256,1,1)f32 #147=(1,256,1,1)f32 #148=(1,256,50,50)f32
nn.ReLU                  m.backbone.body.layer3.1.relu 1 1 148 149 #148=(1,256,50,50)f32 #149=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.1.conv2 1 1 149 150 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(256,256,3,3)f32 #149=(1,256,50,50)f32 #150=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.66       0 1 151 @data=(1,256,1,1)f32 #151=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.66       0 1 152 @data=(1,256,1,1)f32 #152=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1310           3 1 150 151 152 153 expr=add(mul(@0,@1),@2) #150=(1,256,50,50)f32 #151=(1,256,1,1)f32 #152=(1,256,1,1)f32 #153=(1,256,50,50)f32
nn.ReLU                  pnnx_unique_106          1 1 153 154 #153=(1,256,50,50)f32 #154=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.1.conv3 1 1 154 155 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=1024 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(1024,256,1,1)f32 #154=(1,256,50,50)f32 #155=(1,1024,50,50)f32
pnnx.Attribute           pnnx_fold_scale.68       0 1 156 @data=(1,1024,1,1)f32 #156=(1,1024,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.68       0 1 157 @data=(1,1024,1,1)f32 #157=(1,1024,1,1)f32
pnnx.Expression          pnnx_expr_1297           4 1 155 156 157 144 158 expr=add(add(mul(@0,@1),@2),@3) #155=(1,1024,50,50)f32 #156=(1,1024,1,1)f32 #157=(1,1024,1,1)f32 #144=(1,1024,50,50)f32 #158=(1,1024,50,50)f32
nn.ReLU                  pnnx_unique_110          1 1 158 159 #158=(1,1024,50,50)f32 #159=(1,1024,50,50)f32
nn.Conv2d                m.backbone.body.layer3.2.conv1 1 1 159 160 bias=False dilation=(1,1) groups=1 in_channels=1024 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,1024,1,1)f32 #159=(1,1024,50,50)f32 #160=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.15       0 1 161 @data=(1,256,1,1)f32 #161=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.15       0 1 162 @data=(1,256,1,1)f32 #162=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1284           3 1 160 161 162 163 expr=add(mul(@0,@1),@2) #160=(1,256,50,50)f32 #161=(1,256,1,1)f32 #162=(1,256,1,1)f32 #163=(1,256,50,50)f32
nn.ReLU                  m.backbone.body.layer3.2.relu 1 1 163 164 #163=(1,256,50,50)f32 #164=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.2.conv2 1 1 164 165 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(256,256,3,3)f32 #164=(1,256,50,50)f32 #165=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.17       0 1 166 @data=(1,256,1,1)f32 #166=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.17       0 1 167 @data=(1,256,1,1)f32 #167=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1272           3 1 165 166 167 168 expr=add(mul(@0,@1),@2) #165=(1,256,50,50)f32 #166=(1,256,1,1)f32 #167=(1,256,1,1)f32 #168=(1,256,50,50)f32
nn.ReLU                  pnnx_unique_117          1 1 168 169 #168=(1,256,50,50)f32 #169=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.2.conv3 1 1 169 170 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=1024 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(1024,256,1,1)f32 #169=(1,256,50,50)f32 #170=(1,1024,50,50)f32
pnnx.Attribute           pnnx_fold_scale.19       0 1 171 @data=(1,1024,1,1)f32 #171=(1,1024,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.19       0 1 172 @data=(1,1024,1,1)f32 #172=(1,1024,1,1)f32
pnnx.Expression          pnnx_expr_1259           4 1 170 171 172 159 173 expr=add(add(mul(@0,@1),@2),@3) #170=(1,1024,50,50)f32 #171=(1,1024,1,1)f32 #172=(1,1024,1,1)f32 #159=(1,1024,50,50)f32 #173=(1,1024,50,50)f32
nn.ReLU                  pnnx_unique_121          1 1 173 174 #173=(1,1024,50,50)f32 #174=(1,1024,50,50)f32
nn.Conv2d                m.backbone.body.layer3.3.conv1 1 1 174 175 bias=False dilation=(1,1) groups=1 in_channels=1024 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,1024,1,1)f32 #174=(1,1024,50,50)f32 #175=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.21       0 1 176 @data=(1,256,1,1)f32 #176=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.21       0 1 177 @data=(1,256,1,1)f32 #177=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1246           3 1 175 176 177 178 expr=add(mul(@0,@1),@2) #175=(1,256,50,50)f32 #176=(1,256,1,1)f32 #177=(1,256,1,1)f32 #178=(1,256,50,50)f32
nn.ReLU                  m.backbone.body.layer3.3.relu 1 1 178 179 #178=(1,256,50,50)f32 #179=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.3.conv2 1 1 179 180 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(256,256,3,3)f32 #179=(1,256,50,50)f32 #180=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.23       0 1 181 @data=(1,256,1,1)f32 #181=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.23       0 1 182 @data=(1,256,1,1)f32 #182=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1234           3 1 180 181 182 183 expr=add(mul(@0,@1),@2) #180=(1,256,50,50)f32 #181=(1,256,1,1)f32 #182=(1,256,1,1)f32 #183=(1,256,50,50)f32
nn.ReLU                  pnnx_unique_128          1 1 183 184 #183=(1,256,50,50)f32 #184=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.3.conv3 1 1 184 185 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=1024 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(1024,256,1,1)f32 #184=(1,256,50,50)f32 #185=(1,1024,50,50)f32
pnnx.Attribute           pnnx_fold_scale.25       0 1 186 @data=(1,1024,1,1)f32 #186=(1,1024,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.25       0 1 187 @data=(1,1024,1,1)f32 #187=(1,1024,1,1)f32
pnnx.Expression          pnnx_expr_1221           4 1 185 186 187 174 188 expr=add(add(mul(@0,@1),@2),@3) #185=(1,1024,50,50)f32 #186=(1,1024,1,1)f32 #187=(1,1024,1,1)f32 #174=(1,1024,50,50)f32 #188=(1,1024,50,50)f32
nn.ReLU                  pnnx_unique_132          1 1 188 189 #188=(1,1024,50,50)f32 #189=(1,1024,50,50)f32
nn.Conv2d                m.backbone.body.layer3.4.conv1 1 1 189 190 bias=False dilation=(1,1) groups=1 in_channels=1024 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,1024,1,1)f32 #189=(1,1024,50,50)f32 #190=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.27       0 1 191 @data=(1,256,1,1)f32 #191=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.27       0 1 192 @data=(1,256,1,1)f32 #192=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1208           3 1 190 191 192 193 expr=add(mul(@0,@1),@2) #190=(1,256,50,50)f32 #191=(1,256,1,1)f32 #192=(1,256,1,1)f32 #193=(1,256,50,50)f32
nn.ReLU                  m.backbone.body.layer3.4.relu 1 1 193 194 #193=(1,256,50,50)f32 #194=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.4.conv2 1 1 194 195 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(256,256,3,3)f32 #194=(1,256,50,50)f32 #195=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.29       0 1 196 @data=(1,256,1,1)f32 #196=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.29       0 1 197 @data=(1,256,1,1)f32 #197=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1196           3 1 195 196 197 198 expr=add(mul(@0,@1),@2) #195=(1,256,50,50)f32 #196=(1,256,1,1)f32 #197=(1,256,1,1)f32 #198=(1,256,50,50)f32
nn.ReLU                  pnnx_unique_139          1 1 198 199 #198=(1,256,50,50)f32 #199=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.4.conv3 1 1 199 200 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=1024 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(1024,256,1,1)f32 #199=(1,256,50,50)f32 #200=(1,1024,50,50)f32
pnnx.Attribute           pnnx_fold_scale.31       0 1 201 @data=(1,1024,1,1)f32 #201=(1,1024,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.31       0 1 202 @data=(1,1024,1,1)f32 #202=(1,1024,1,1)f32
pnnx.Expression          pnnx_expr_1183           4 1 200 201 202 189 203 expr=add(add(mul(@0,@1),@2),@3) #200=(1,1024,50,50)f32 #201=(1,1024,1,1)f32 #202=(1,1024,1,1)f32 #189=(1,1024,50,50)f32 #203=(1,1024,50,50)f32
nn.ReLU                  pnnx_unique_143          1 1 203 204 #203=(1,1024,50,50)f32 #204=(1,1024,50,50)f32
nn.Conv2d                m.backbone.body.layer3.5.conv1 1 1 204 205 bias=False dilation=(1,1) groups=1 in_channels=1024 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(256,1024,1,1)f32 #204=(1,1024,50,50)f32 #205=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.70       0 1 206 @data=(1,256,1,1)f32 #206=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.70       0 1 207 @data=(1,256,1,1)f32 #207=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1170           3 1 205 206 207 208 expr=add(mul(@0,@1),@2) #205=(1,256,50,50)f32 #206=(1,256,1,1)f32 #207=(1,256,1,1)f32 #208=(1,256,50,50)f32
nn.ReLU                  m.backbone.body.layer3.5.relu 1 1 208 209 #208=(1,256,50,50)f32 #209=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.5.conv2 1 1 209 210 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(256,256,3,3)f32 #209=(1,256,50,50)f32 #210=(1,256,50,50)f32
pnnx.Attribute           pnnx_fold_scale.72       0 1 211 @data=(1,256,1,1)f32 #211=(1,256,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.72       0 1 212 @data=(1,256,1,1)f32 #212=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_1158           3 1 210 211 212 213 expr=add(mul(@0,@1),@2) #210=(1,256,50,50)f32 #211=(1,256,1,1)f32 #212=(1,256,1,1)f32 #213=(1,256,50,50)f32
nn.ReLU                  pnnx_unique_150          1 1 213 214 #213=(1,256,50,50)f32 #214=(1,256,50,50)f32
nn.Conv2d                m.backbone.body.layer3.5.conv3 1 1 214 215 bias=False dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=1024 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(1024,256,1,1)f32 #214=(1,256,50,50)f32 #215=(1,1024,50,50)f32
pnnx.Attribute           pnnx_fold_scale.74       0 1 216 @data=(1,1024,1,1)f32 #216=(1,1024,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.74       0 1 217 @data=(1,1024,1,1)f32 #217=(1,1024,1,1)f32
pnnx.Expression          pnnx_expr_1145           4 1 215 216 217 204 218 expr=add(add(mul(@0,@1),@2),@3) #215=(1,1024,50,50)f32 #216=(1,1024,1,1)f32 #217=(1,1024,1,1)f32 #204=(1,1024,50,50)f32 #218=(1,1024,50,50)f32
nn.ReLU                  pnnx_unique_154          1 1 218 219 #218=(1,1024,50,50)f32 #219=(1,1024,50,50)f32
nn.Conv2d                m.backbone.body.layer4.0.conv1 1 1 219 220 bias=False dilation=(1,1) groups=1 in_channels=1024 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(512,1024,1,1)f32 #219=(1,1024,50,50)f32 #220=(1,512,50,50)f32
pnnx.Attribute           pnnx_fold_scale.3        0 1 221 @data=(1,512,1,1)f32 #221=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.3        0 1 222 @data=(1,512,1,1)f32 #222=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1132           3 1 220 221 222 223 expr=add(mul(@0,@1),@2) #220=(1,512,50,50)f32 #221=(1,512,1,1)f32 #222=(1,512,1,1)f32 #223=(1,512,50,50)f32
nn.ReLU                  m.backbone.body.layer4.0.relu 1 1 223 224 #223=(1,512,50,50)f32 #224=(1,512,50,50)f32
nn.Conv2d                m.backbone.body.layer4.0.conv2 1 1 224 225 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(3,3) out_channels=512 padding=(1,1) padding_mode=zeros stride=(2,2) @weight=(512,512,3,3)f32 #224=(1,512,50,50)f32 #225=(1,512,25,25)f32
pnnx.Attribute           pnnx_fold_scale.5        0 1 226 @data=(1,512,1,1)f32 #226=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.5        0 1 227 @data=(1,512,1,1)f32 #227=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1120           3 1 225 226 227 228 expr=add(mul(@0,@1),@2) #225=(1,512,25,25)f32 #226=(1,512,1,1)f32 #227=(1,512,1,1)f32 #228=(1,512,25,25)f32
nn.ReLU                  pnnx_unique_161          1 1 228 229 #228=(1,512,25,25)f32 #229=(1,512,25,25)f32
nn.Conv2d                m.backbone.body.layer4.0.conv3 1 1 229 230 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=2048 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(2048,512,1,1)f32 #229=(1,512,25,25)f32 #230=(1,2048,25,25)f32
pnnx.Attribute           pnnx_fold_scale.6        0 1 231 @data=(1,2048,1,1)f32 #231=(1,2048,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.6        0 1 232 @data=(1,2048,1,1)f32 #232=(1,2048,1,1)f32
nn.Conv2d                m.backbone.body.layer4.0.downsample.0 1 1 219 233 bias=False dilation=(1,1) groups=1 in_channels=1024 kernel_size=(1,1) out_channels=2048 padding=(0,0) padding_mode=zeros stride=(2,2) @weight=(2048,1024,1,1)f32 #219=(1,1024,50,50)f32 #233=(1,2048,25,25)f32
pnnx.Attribute           pnnx_fold_scale.7        0 1 234 @data=(1,2048,1,1)f32 #234=(1,2048,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.7        0 1 235 @data=(1,2048,1,1)f32 #235=(1,2048,1,1)f32
pnnx.Expression          pnnx_expr_1095           6 1 230 231 232 233 234 235 236 expr=add(add(mul(@0,@1),@2),add(mul(@3,@4),@5)) #230=(1,2048,25,25)f32 #231=(1,2048,1,1)f32 #232=(1,2048,1,1)f32 #233=(1,2048,25,25)f32 #234=(1,2048,1,1)f32 #235=(1,2048,1,1)f32 #236=(1,2048,25,25)f32
nn.ReLU                  pnnx_unique_168          1 1 236 237 #236=(1,2048,25,25)f32 #237=(1,2048,25,25)f32
nn.Conv2d                m.backbone.body.layer4.1.conv1 1 1 237 238 bias=False dilation=(1,1) groups=1 in_channels=2048 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(512,2048,1,1)f32 #237=(1,2048,25,25)f32 #238=(1,512,25,25)f32
pnnx.Attribute           pnnx_fold_scale.9        0 1 239 @data=(1,512,1,1)f32 #239=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.9        0 1 240 @data=(1,512,1,1)f32 #240=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1082           3 1 238 239 240 241 expr=add(mul(@0,@1),@2) #238=(1,512,25,25)f32 #239=(1,512,1,1)f32 #240=(1,512,1,1)f32 #241=(1,512,25,25)f32
nn.ReLU                  m.backbone.body.layer4.1.relu 1 1 241 242 #241=(1,512,25,25)f32 #242=(1,512,25,25)f32
nn.Conv2d                m.backbone.body.layer4.1.conv2 1 1 242 243 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(3,3) out_channels=512 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(512,512,3,3)f32 #242=(1,512,25,25)f32 #243=(1,512,25,25)f32
pnnx.Attribute           pnnx_fold_scale.11       0 1 244 @data=(1,512,1,1)f32 #244=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.11       0 1 245 @data=(1,512,1,1)f32 #245=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1070           3 1 243 244 245 246 expr=add(mul(@0,@1),@2) #243=(1,512,25,25)f32 #244=(1,512,1,1)f32 #245=(1,512,1,1)f32 #246=(1,512,25,25)f32
nn.ReLU                  pnnx_unique_175          1 1 246 247 #246=(1,512,25,25)f32 #247=(1,512,25,25)f32
nn.Conv2d                m.backbone.body.layer4.1.conv3 1 1 247 248 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=2048 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(2048,512,1,1)f32 #247=(1,512,25,25)f32 #248=(1,2048,25,25)f32
pnnx.Attribute           pnnx_fold_scale.13       0 1 249 @data=(1,2048,1,1)f32 #249=(1,2048,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.13       0 1 250 @data=(1,2048,1,1)f32 #250=(1,2048,1,1)f32
pnnx.Expression          pnnx_expr_1057           4 1 248 249 250 237 251 expr=add(add(mul(@0,@1),@2),@3) #248=(1,2048,25,25)f32 #249=(1,2048,1,1)f32 #250=(1,2048,1,1)f32 #237=(1,2048,25,25)f32 #251=(1,2048,25,25)f32
nn.ReLU                  pnnx_unique_179          1 1 251 252 #251=(1,2048,25,25)f32 #252=(1,2048,25,25)f32
nn.Conv2d                m.backbone.body.layer4.2.conv1 1 1 252 253 bias=False dilation=(1,1) groups=1 in_channels=2048 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(512,2048,1,1)f32 #252=(1,2048,25,25)f32 #253=(1,512,25,25)f32
pnnx.Attribute           pnnx_fold_scale.2        0 1 254 @data=(1,512,1,1)f32 #254=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.2        0 1 255 @data=(1,512,1,1)f32 #255=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1044           3 1 253 254 255 256 expr=add(mul(@0,@1),@2) #253=(1,512,25,25)f32 #254=(1,512,1,1)f32 #255=(1,512,1,1)f32 #256=(1,512,25,25)f32
nn.ReLU                  m.backbone.body.layer4.2.relu 1 1 256 257 #256=(1,512,25,25)f32 #257=(1,512,25,25)f32
nn.Conv2d                m.backbone.body.layer4.2.conv2 1 1 257 258 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(3,3) out_channels=512 padding=(1,1) padding_mode=zeros stride=(1,1) @weight=(512,512,3,3)f32 #257=(1,512,25,25)f32 #258=(1,512,25,25)f32
pnnx.Attribute           pnnx_fold_scale.4        0 1 259 @data=(1,512,1,1)f32 #259=(1,512,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.4        0 1 260 @data=(1,512,1,1)f32 #260=(1,512,1,1)f32
pnnx.Expression          pnnx_expr_1032           3 1 258 259 260 261 expr=add(mul(@0,@1),@2) #258=(1,512,25,25)f32 #259=(1,512,1,1)f32 #260=(1,512,1,1)f32 #261=(1,512,25,25)f32
nn.ReLU                  pnnx_unique_186          1 1 261 262 #261=(1,512,25,25)f32 #262=(1,512,25,25)f32
nn.Conv2d                m.backbone.body.layer4.2.conv3 1 1 262 263 bias=False dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=2048 padding=(0,0) padding_mode=zeros stride=(1,1) @weight=(2048,512,1,1)f32 #262=(1,512,25,25)f32 #263=(1,2048,25,25)f32
pnnx.Attribute           pnnx_fold_scale.1        0 1 264 @data=(1,2048,1,1)f32 #264=(1,2048,1,1)f32
pnnx.Attribute           pnnx_fold_bias0.1        0 1 265 @data=(1,2048,1,1)f32 #265=(1,2048,1,1)f32
pnnx.Expression          pnnx_expr_1019           4 1 263 264 265 252 266 expr=add(add(mul(@0,@1),@2),@3) #263=(1,2048,25,25)f32 #264=(1,2048,1,1)f32 #265=(1,2048,1,1)f32 #252=(1,2048,25,25)f32 #266=(1,2048,25,25)f32
nn.ReLU                  pnnx_unique_190          1 1 266 267 #266=(1,2048,25,25)f32 #267=(1,2048,25,25)f32
nn.Conv2d                m.backbone.fpn.inner_blocks.3.0 1 1 267 268 bias=True dilation=(1,1) groups=1 in_channels=2048 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,2048,1,1)f32 #267=(1,2048,25,25)f32 #268=(1,256,25,25)f32
nn.Conv2d                m.backbone.fpn.layer_blocks.3.0 1 1 268 269 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #268=(1,256,25,25)f32 #269=(1,256,25,25)f32
nn.Conv2d                m.backbone.fpn.inner_blocks.2.0 1 1 219 270 bias=True dilation=(1,1) groups=1 in_channels=1024 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1024,1,1)f32 #219=(1,1024,50,50)f32 #270=(1,256,50,50)f32
F.upsample_nearest       F.upsample_nearest_718   1 1 268 271 size=(50,50) $input=268 #268=(1,256,25,25)f32 #271=(1,256,50,50)f32
pnnx.Expression          pnnx_expr_1014           2 1 270 271 272 expr=add(@0,@1) #270=(1,256,50,50)f32 #271=(1,256,50,50)f32 #272=(1,256,50,50)f32
nn.Conv2d                m.backbone.fpn.layer_blocks.2.0 1 1 272 273 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #272=(1,256,50,50)f32 #273=(1,256,50,50)f32
nn.Conv2d                m.backbone.fpn.inner_blocks.1.0 1 1 126 274 bias=True dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,512,1,1)f32 #126=(1,512,100,100)f32 #274=(1,256,100,100)f32
F.upsample_nearest       F.upsample_nearest_719   1 1 272 275 size=(100,100) $input=272 #272=(1,256,50,50)f32 #275=(1,256,100,100)f32
pnnx.Expression          pnnx_expr_1009           2 1 274 275 276 expr=add(@0,@1) #274=(1,256,100,100)f32 #275=(1,256,100,100)f32 #276=(1,256,100,100)f32
nn.Conv2d                m.backbone.fpn.layer_blocks.1.0 1 1 276 277 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #276=(1,256,100,100)f32 #277=(1,256,100,100)f32
nn.Conv2d                m.backbone.fpn.inner_blocks.0.0 1 1 63 278 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #63=(1,256,200,200)f32 #278=(1,256,200,200)f32
F.upsample_nearest       F.upsample_nearest_720   1 1 276 279 size=(200,200) $input=276 #276=(1,256,100,100)f32 #279=(1,256,200,200)f32
pnnx.Expression          pnnx_expr_1004           2 1 278 279 280 expr=add(@0,@1) #278=(1,256,200,200)f32 #279=(1,256,200,200)f32 #280=(1,256,200,200)f32
nn.Conv2d                m.backbone.fpn.layer_blocks.0.0 1 1 280 281 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #280=(1,256,200,200)f32 #281=(1,256,200,200)f32
F.max_pool2d             F.max_pool2d_723         1 1 269 282 ceil_mode=False dilation=(1,1) kernel_size=(1,1) padding=(0,0) return_indices=False stride=(2,2) $input=269 #269=(1,256,25,25)f32 #282=(1,256,13,13)f32
nn.Conv2d                m.rpn.head.conv.0.0      1 1 281 283 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #281=(1,256,200,200)f32 #283=(1,256,200,200)f32
nn.ReLU                  m.rpn.head.conv.0.1      1 1 283 284 #283=(1,256,200,200)f32 #284=(1,256,200,200)f32
nn.Conv2d                m.rpn.head.cls_logits    1 1 284 285 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=3 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(3)f32 @weight=(3,256,1,1)f32 #284=(1,256,200,200)f32 #285=(1,3,200,200)f32
nn.Conv2d                m.rpn.head.bbox_pred     1 1 284 286 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=12 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(12)f32 @weight=(12,256,1,1)f32 #284=(1,256,200,200)f32 #286=(1,12,200,200)f32
nn.Conv2d                pnnx_unique_191          1 1 277 287 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #277=(1,256,100,100)f32 #287=(1,256,100,100)f32
nn.ReLU                  pnnx_unique_192          1 1 287 288 #287=(1,256,100,100)f32 #288=(1,256,100,100)f32
nn.Conv2d                pnnx_unique_193          1 1 288 289 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=3 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(3)f32 @weight=(3,256,1,1)f32 #288=(1,256,100,100)f32 #289=(1,3,100,100)f32
nn.Conv2d                pnnx_unique_194          1 1 288 290 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=12 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(12)f32 @weight=(12,256,1,1)f32 #288=(1,256,100,100)f32 #290=(1,12,100,100)f32
nn.Conv2d                pnnx_unique_195          1 1 273 291 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #273=(1,256,50,50)f32 #291=(1,256,50,50)f32
nn.ReLU                  pnnx_unique_196          1 1 291 292 #291=(1,256,50,50)f32 #292=(1,256,50,50)f32
nn.Conv2d                pnnx_unique_197          1 1 292 293 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=3 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(3)f32 @weight=(3,256,1,1)f32 #292=(1,256,50,50)f32 #293=(1,3,50,50)f32
nn.Conv2d                pnnx_unique_198          1 1 292 294 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=12 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(12)f32 @weight=(12,256,1,1)f32 #292=(1,256,50,50)f32 #294=(1,12,50,50)f32
nn.Conv2d                pnnx_unique_199          1 1 269 295 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #269=(1,256,25,25)f32 #295=(1,256,25,25)f32
nn.ReLU                  pnnx_unique_200          1 1 295 296 #295=(1,256,25,25)f32 #296=(1,256,25,25)f32
nn.Conv2d                pnnx_unique_201          1 1 296 297 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=3 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(3)f32 @weight=(3,256,1,1)f32 #296=(1,256,25,25)f32 #297=(1,3,25,25)f32
nn.Conv2d                pnnx_unique_202          1 1 296 298 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=12 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(12)f32 @weight=(12,256,1,1)f32 #296=(1,256,25,25)f32 #298=(1,12,25,25)f32
nn.Conv2d                pnnx_unique_203          1 1 282 299 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #282=(1,256,13,13)f32 #299=(1,256,13,13)f32
nn.ReLU                  pnnx_unique_204          1 1 299 300 #299=(1,256,13,13)f32 #300=(1,256,13,13)f32
nn.Conv2d                pnnx_unique_205          1 1 300 301 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=3 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(3)f32 @weight=(3,256,1,1)f32 #300=(1,256,13,13)f32 #301=(1,3,13,13)f32
nn.Conv2d                pnnx_unique_206          1 1 300 302 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=12 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(12)f32 @weight=(12,256,1,1)f32 #300=(1,256,13,13)f32 #302=(1,12,13,13)f32
Tensor.reshape           Tensor.reshape_515       1 1 285 303 shape=(1,3,1,200,200) $input=285 #285=(1,3,200,200)f32 #303=(1,3,1,200,200)f32
Tensor.permute           Tensor.permute_246       1 1 303 304 dims=(0,3,4,1,2) $input=303 #303=(1,3,1,200,200)f32 #304=(1,200,200,3,1)f32
Tensor.reshape           Tensor.reshape_483       1 1 304 305 shape=(1,120000,1) $input=304 #304=(1,200,200,3,1)f32 #305=(1,120000,1)f32
Tensor.reshape           Tensor.reshape_516       1 1 286 306 shape=(1,3,4,200,200) $input=286 #286=(1,12,200,200)f32 #306=(1,3,4,200,200)f32
Tensor.permute           Tensor.permute_247       1 1 306 307 dims=(0,3,4,1,2) $input=306 #306=(1,3,4,200,200)f32 #307=(1,200,200,3,4)f32
Tensor.reshape           Tensor.reshape_484       1 1 307 308 shape=(1,120000,4) $input=307 #307=(1,200,200,3,4)f32 #308=(1,120000,4)f32
Tensor.reshape           Tensor.reshape_517       1 1 289 309 shape=(1,3,1,100,100) $input=289 #289=(1,3,100,100)f32 #309=(1,3,1,100,100)f32
Tensor.permute           Tensor.permute_248       1 1 309 310 dims=(0,3,4,1,2) $input=309 #309=(1,3,1,100,100)f32 #310=(1,100,100,3,1)f32
Tensor.reshape           Tensor.reshape_485       1 1 310 311 shape=(1,30000,1) $input=310 #310=(1,100,100,3,1)f32 #311=(1,30000,1)f32
Tensor.reshape           Tensor.reshape_518       1 1 290 312 shape=(1,3,4,100,100) $input=290 #290=(1,12,100,100)f32 #312=(1,3,4,100,100)f32
Tensor.permute           Tensor.permute_249       1 1 312 313 dims=(0,3,4,1,2) $input=312 #312=(1,3,4,100,100)f32 #313=(1,100,100,3,4)f32
Tensor.reshape           Tensor.reshape_486       1 1 313 314 shape=(1,30000,4) $input=313 #313=(1,100,100,3,4)f32 #314=(1,30000,4)f32
Tensor.reshape           Tensor.reshape_519       1 1 293 315 shape=(1,3,1,50,50) $input=293 #293=(1,3,50,50)f32 #315=(1,3,1,50,50)f32
Tensor.permute           Tensor.permute_250       1 1 315 316 dims=(0,3,4,1,2) $input=315 #315=(1,3,1,50,50)f32 #316=(1,50,50,3,1)f32
Tensor.reshape           Tensor.reshape_487       1 1 316 317 shape=(1,7500,1) $input=316 #316=(1,50,50,3,1)f32 #317=(1,7500,1)f32
Tensor.reshape           Tensor.reshape_520       1 1 294 318 shape=(1,3,4,50,50) $input=294 #294=(1,12,50,50)f32 #318=(1,3,4,50,50)f32
Tensor.permute           Tensor.permute_251       1 1 318 319 dims=(0,3,4,1,2) $input=318 #318=(1,3,4,50,50)f32 #319=(1,50,50,3,4)f32
Tensor.reshape           Tensor.reshape_488       1 1 319 320 shape=(1,7500,4) $input=319 #319=(1,50,50,3,4)f32 #320=(1,7500,4)f32
Tensor.reshape           Tensor.reshape_521       1 1 297 321 shape=(1,3,1,25,25) $input=297 #297=(1,3,25,25)f32 #321=(1,3,1,25,25)f32
Tensor.permute           Tensor.permute_252       1 1 321 322 dims=(0,3,4,1,2) $input=321 #321=(1,3,1,25,25)f32 #322=(1,25,25,3,1)f32
Tensor.reshape           Tensor.reshape_489       1 1 322 323 shape=(1,1875,1) $input=322 #322=(1,25,25,3,1)f32 #323=(1,1875,1)f32
Tensor.reshape           Tensor.reshape_522       1 1 298 324 shape=(1,3,4,25,25) $input=298 #298=(1,12,25,25)f32 #324=(1,3,4,25,25)f32
Tensor.permute           Tensor.permute_253       1 1 324 325 dims=(0,3,4,1,2) $input=324 #324=(1,3,4,25,25)f32 #325=(1,25,25,3,4)f32
Tensor.reshape           Tensor.reshape_490       1 1 325 326 shape=(1,1875,4) $input=325 #325=(1,25,25,3,4)f32 #326=(1,1875,4)f32
Tensor.reshape           Tensor.reshape_523       1 1 301 327 shape=(1,3,1,13,13) $input=301 #301=(1,3,13,13)f32 #327=(1,3,1,13,13)f32
Tensor.permute           Tensor.permute_254       1 1 327 328 dims=(0,3,4,1,2) $input=327 #327=(1,3,1,13,13)f32 #328=(1,13,13,3,1)f32
Tensor.reshape           Tensor.reshape_491       1 1 328 329 shape=(1,507,1) $input=328 #328=(1,13,13,3,1)f32 #329=(1,507,1)f32
Tensor.reshape           Tensor.reshape_524       1 1 302 330 shape=(1,3,4,13,13) $input=302 #302=(1,12,13,13)f32 #330=(1,3,4,13,13)f32
Tensor.permute           Tensor.permute_255       1 1 330 331 dims=(0,3,4,1,2) $input=330 #330=(1,3,4,13,13)f32 #331=(1,13,13,3,4)f32
Tensor.reshape           Tensor.reshape_492       1 1 331 332 shape=(1,507,4) $input=331 #331=(1,13,13,3,4)f32 #332=(1,507,4)f32
torch.cat                torch.cat_210            5 1 305 311 317 323 329 333 dim=1 #305=(1,120000,1)f32 #311=(1,30000,1)f32 #317=(1,7500,1)f32 #323=(1,1875,1)f32 #329=(1,507,1)f32 #333=(1,159882,1)f32
torch.flatten            torch.flatten_536        1 1 333 334 end_dim=-2 start_dim=0 $input=333 #333=(1,159882,1)f32 #334=(159882,1)f32
torch.cat                torch.cat_211            5 1 308 314 320 326 332 335 dim=1 #308=(1,120000,4)f32 #314=(1,30000,4)f32 #320=(1,7500,4)f32 #326=(1,1875,4)f32 #332=(1,507,4)f32 #335=(1,159882,4)f32
Tensor.reshape           Tensor.reshape_494       1 1 335 336 shape=(159882,4) $input=335 #335=(1,159882,4)f32 #336=(159882,4)f32
Tensor.slice             Tensor.slice_664         1 1 336 337 dim=1 end=2147483647 start=0 step=4 $input=336 #336=(159882,4)f32 #337=(159882,1)f32
Tensor.slice             Tensor.slice_666         1 1 336 338 dim=1 end=2147483647 start=1 step=4 $input=336 #336=(159882,4)f32 #338=(159882,1)f32
Tensor.slice             Tensor.slice_668         1 1 336 339 dim=1 end=2147483647 start=2 step=4 $input=336 #336=(159882,4)f32 #339=(159882,1)f32
Tensor.slice             Tensor.slice_670         1 1 336 340 dim=1 end=2147483647 start=3 step=4 $input=336 #336=(159882,4)f32 #340=(159882,1)f32
torch.clamp              torch.clamp_149          1 1 339 341 max=4.13517 min=None $input=339 #339=(159882,1)f32 #341=(159882,1)f32
torch.clamp              torch.clamp_150          1 1 340 342 max=4.13517 min=None $input=340 #340=(159882,1)f32 #342=(159882,1)f32
pnnx.Attribute           pnnx_fold_2258           0 1 343 @data=(159882,1)f32 #343=(159882,1)f32
pnnx.Attribute           pnnx_fold_2261           0 1 344 @data=(159882,1)f32 #344=(159882,1)f32
pnnx.Expression          pnnx_expr_582            3 1 337 343 344 345 expr=add(mul(@0,@1),@2) #337=(159882,1)f32 #343=(159882,1)f32 #344=(159882,1)f32 #345=(159882,1)f32
pnnx.Attribute           pnnx_fold_2264           0 1 346 @data=(159882,1)f32 #346=(159882,1)f32
pnnx.Attribute           pnnx_fold_2267           0 1 347 @data=(159882,1)f32 #347=(159882,1)f32
pnnx.Expression          pnnx_expr_571            3 1 338 346 347 348 expr=add(mul(@0,@1),@2) #338=(159882,1)f32 #346=(159882,1)f32 #347=(159882,1)f32 #348=(159882,1)f32
pnnx.Attribute           pnnx_fold_2271           0 1 349 @data=(159882,1)f32 #349=(159882,1)f32
pnnx.Attribute           pnnx_fold_2275           0 1 350 @data=(159882,1)f32 #350=(159882,1)f32
pnnx.Expression          pnnx_expr_556            2 1 342 350 351 expr=mul(0.5,mul(exp(@0),@1)) #342=(159882,1)f32 #350=(159882,1)f32 #351=(159882,1)f32
pnnx.Expression          pnnx_expr_552            2 1 341 349 352 expr=mul(0.5,mul(exp(@0),@1)) #341=(159882,1)f32 #349=(159882,1)f32 #352=(159882,1)f32
pnnx.Expression          pnnx_expr_550            2 1 345 352 353 expr=sub(@0,@1) #345=(159882,1)f32 #352=(159882,1)f32 #353=(159882,1)f32
pnnx.Expression          pnnx_expr_548            2 1 348 351 354 expr=sub(@0,@1) #348=(159882,1)f32 #351=(159882,1)f32 #354=(159882,1)f32
pnnx.Expression          pnnx_expr_546            2 1 345 352 355 expr=add(@0,@1) #345=(159882,1)f32 #352=(159882,1)f32 #355=(159882,1)f32
pnnx.Expression          pnnx_expr_544            2 1 348 351 356 expr=add(@0,@1) #348=(159882,1)f32 #351=(159882,1)f32 #356=(159882,1)f32
torch.stack              torch.stack_237          4 1 353 354 355 356 357 dim=2 #353=(159882,1)f32 #354=(159882,1)f32 #355=(159882,1)f32 #356=(159882,1)f32 #357=(159882,1,4)f32
torch.flatten            torch.flatten_537        1 1 357 358 end_dim=-1 start_dim=1 $input=357 #357=(159882,1,4)f32 #358=(159882,4)f32
Tensor.reshape           Tensor.reshape_526       1 1 358 359 shape=(1,159882,4) $input=358 #358=(159882,4)f32 #359=(1,159882,4)f32
Tensor.reshape           Tensor.reshape_496       1 1 334 360 shape=(1,159882) $input=334 #334=(159882,1)f32 #360=(1,159882)f32
torch.split              torch.split_225          1 5 360 361 362 363 364 365 dim=1 split_size_or_sections=(120000,30000,7500,1875,507) $tensor=360 #360=(1,159882)f32 #361=(1,120000)f32 #362=(1,30000)f32 #363=(1,7500)f32 #364=(1,1875)f32 #365=(1,507)f32
torch.topk               torch.topk_154           1 2 361 366 367 dim=1 k=1000 largest=True sorted=True $input=361 #361=(1,120000)f32 #366=(1,1000)f32 #367=(1,1000)i64
torch.topk               torch.topk_155           1 2 362 368 369 dim=1 k=1000 largest=True sorted=True $input=362 #362=(1,30000)f32 #368=(1,1000)f32 #369=(1,1000)i64
pnnx.Expression          pnnx_expr_505            1 1 369 370 expr=add(@0,120000) #369=(1,1000)i64 #370=(1,1000)i64
torch.topk               torch.topk_156           1 2 363 371 372 dim=1 k=1000 largest=True sorted=True $input=363 #363=(1,7500)f32 #371=(1,1000)f32 #372=(1,1000)i64
pnnx.Expression          pnnx_expr_490            1 1 372 373 expr=add(@0,150000) #372=(1,1000)i64 #373=(1,1000)i64
torch.topk               torch.topk_157           1 2 364 374 375 dim=1 k=1000 largest=True sorted=True $input=364 #364=(1,1875)f32 #374=(1,1000)f32 #375=(1,1000)i64
pnnx.Expression          pnnx_expr_475            1 1 375 376 expr=add(@0,157500) #375=(1,1000)i64 #376=(1,1000)i64
torch.topk               torch.topk_158           1 2 365 377 378 dim=1 k=507 largest=True sorted=True $input=365 #365=(1,507)f32 #377=(1,507)f32 #378=(1,507)i64
pnnx.Expression          pnnx_expr_462            1 1 378 379 expr=add(@0,159375) #378=(1,507)i64 #379=(1,507)i64
torch.cat                torch.cat_219            5 1 367 370 373 376 379 380 dim=1 #367=(1,1000)i64 #370=(1,1000)i64 #373=(1,1000)i64 #376=(1,1000)i64 #379=(1,507)i64 #380=(1,4507)i64
pnnx.Attribute           pnnx_fold_batch_idx.1    0 1 381 @data=(1,1)i64 #381=(1,1)i64
pnnx.Expression          pnnx_expr_456            2 1 381 380 382 expr=[@0,@1] #381=(1,1)i64 #380=(1,4507)i64
Tensor.index             Tensor.index_574         2 1 360 382 383 $input=360 $expr=382 #360=(1,159882)f32 #383=(1,4507)f32
Tensor.index             Tensor.index_576         2 1 359 382 384 $input=359 $expr=382 #359=(1,159882,4)f32 #384=(1,4507,4)f32
F.sigmoid                F.sigmoid_716            1 1 383 385 $input=383 #383=(1,4507)f32 #385=(1,4507)f32
torch.unbind             torch.unbind_242         1 1 384 386 dim=0 $input=384 #384=(1,4507,4)f32 #386=(4507,4)f32
torch.unbind             torch.unbind_243         1 1 385 387 dim=0 $input=385 #385=(1,4507)f32 #387=(4507)f32
Tensor.slice             Tensor.slice_678         1 1 386 388 dim=1 end=2147483647 start=0 step=2 $input=386 #386=(4507,4)f32 #388=(4507,2)f32
Tensor.slice             Tensor.slice_679         1 1 386 389 dim=1 end=2147483647 start=1 step=2 $input=386 #386=(4507,4)f32 #389=(4507,2)f32
pnnx.Expression          pnnx_expr_438            1 1 388 390 expr=min(max(@0,0.0),800.0) #388=(4507,2)f32 #390=(4507,2)f32
pnnx.Expression          pnnx_expr_430            1 1 389 391 expr=min(max(@0,0.0),800.0) #389=(4507,2)f32 #391=(4507,2)f32
torch.stack              torch.stack_238          2 1 390 391 392 dim=2 #390=(4507,2)f32 #391=(4507,2)f32 #392=(4507,2,2)f32
Tensor.reshape           Tensor.reshape_498       1 1 392 393 shape=(4507,4) $input=392 #392=(4507,2,2)f32 #393=(4507,4)f32
torch.unbind             Tensor.select_635        1 4 393 394 395 396 397 dim=1 #393=(4507,4)f32 #394=(4507)f32 #395=(4507)f32 #396=(4507)f32 #397=(4507)f32
pnnx.Expression          pnnx_expr_420            2 1 396 394 398 expr=sub(@0,@1) #396=(4507)f32 #394=(4507)f32 #398=(4507)f32
pnnx.Expression          pnnx_expr_414            2 1 397 395 399 expr=sub(@0,@1) #397=(4507)f32 #395=(4507)f32 #399=(4507)f32
torch.ge                 torch.ge_143             1 1 398 400 other=0.001 $input=398 #398=(4507)f32 #400=(4507)bool
torch.ge                 torch.ge_144             1 1 399 401 other=0.001 $input=399 #399=(4507)f32 #401=(4507)bool
pnnx.Expression          pnnx_expr_412            2 1 400 401 402 expr=and(@0,@1) #400=(4507)bool #401=(4507)bool #402=(4507)bool
aten::where              pnnx_3467                1 1 402 403 #402=(4507)bool
prim::ListUnpack         pnnx_3468                1 1 403 404 #404=(3901)i64
pnnx.Expression          pnnx_expr_411            1 1 404 405 expr=[@0] #404=(3901)i64
Tensor.index             Tensor.index_577         2 1 393 405 406 $input=393 $expr=405 #393=(4507,4)f32 #406=(3901,4)f32
Tensor.index             Tensor.index_578         2 1 387 405 407 $input=387 $expr=405 #387=(4507)f32 #407=(3901)f32
torch.ge                 torch.ge_145             1 1 407 408 other=0.0 $input=407 #407=(3901)f32 #408=(3901)bool
aten::where              pnnx_3476                1 1 408 409 #408=(3901)bool
prim::ListUnpack         pnnx_3477                1 1 409 410 #410=(3901)i64
pnnx.Expression          pnnx_expr_409            1 1 410 411 expr=[@0] #410=(3901)i64
Tensor.index             Tensor.index_580         2 1 406 411 412 $input=406 $expr=411 #406=(3901,4)f32 #412=(3901,4)f32
aten::numel              pnnx_3490                1 1 412 413 #412=(3901,4)f32
torch.eq                 torch.eq_133             1 1 413 414 other=0 $input=413
prim::If                 pnnx_3492                1 1 414 415 #415=(1629)i64
Tensor.slice             Tensor.slice_680         1 1 415 416 dim=0 end=1000 start=0 step=1 $input=415 #415=(1629)i64 #416=(1000)i64
pnnx.Expression          pnnx_expr_404            1 1 416 417 expr=[@0] #416=(1000)i64
Tensor.index             Tensor.index_583         2 1 412 417 418 $input=412 $expr=417 #412=(3901,4)f32 #418=(1000,4)f32
pnnx.Expression          pnnx_expr_376            0 1 419 expr=4
pnnx.Attribute           pnnx_fold_ids.1          0 1 420 @data=(1000,1)f32 #420=(1000,1)f32
torch.cat                torch.cat_222            2 1 420 418 421 dim=1 #420=(1000,1)f32 #418=(1000,4)f32 #421=(1000,5)f32
torch.unbind             Tensor.select_639        1 4 418 422 423 424 425 dim=1 #418=(1000,4)f32 #422=(1000)f32 #423=(1000)f32 #424=(1000)f32 #425=(1000)f32
pnnx.Expression          pnnx_expr_351            4 1 424 422 425 423 426 expr=mul(sub(@0,@1),sub(@2,@3)) #424=(1000)f32 #422=(1000)f32 #425=(1000)f32 #423=(1000)f32 #426=(1000)f32
pnnx.Expression          pnnx_expr_348            1 1 426 427 expr=div(sqrt(@0),224) #426=(1000)f32 #427=(1000)f32
aten::log2               pnnx_3583                1 1 427 428 #427=(1000)f32 #428=(1000)f32
pnnx.Expression          pnnx_expr_341            1 1 428 429 expr=floor(add(add(@0,4),1.000000e-06)) #428=(1000)f32 #429=(1000)f32
torch.clamp              torch.clamp_151          1 1 429 430 max=5 min=2 $input=429 #429=(1000)f32 #430=(1000)f32
aten::to                 pnnx_3599                4 1 430 419 1 1 431 #430=(1000)f32 #431=(1000)i64
pnnx.Expression          pnnx_expr_336            1 1 431 432 expr=sub(@0,2) #431=(1000)i64 #432=(1000)i64
aten::to                 pnnx_3605                4 1 432 419 1 1 433 #432=(1000)i64 #433=(1000)i64
torch.eq                 torch.eq_134             1 1 433 434 other=0 $input=433 #433=(1000)i64 #434=(1000)bool
aten::where              pnnx_3608                1 1 434 435 #434=(1000)bool
prim::ListUnpack         pnnx_3609                1 1 435 436 #436=(619)i64
pnnx.Expression          pnnx_expr_331            1 1 436 437 expr=[@0] #436=(619)i64
Tensor.index             Tensor.index_584         2 1 421 437 438 $input=421 $expr=437 #421=(1000,5)f32 #438=(619,5)f32
pnnx.custom_op.torchvision.roi_align pnnx_3615                2 1 281 438 439 arg_2=0.25 arg_3=7 arg_4=7 arg_5=2 arg_6=False $arg_0=281 $arg_1=438 #281=(1,256,200,200)f32 #438=(619,5)f32 #439=(619,256,7,7)f32
aten::to                 pnnx_3619                4 1 439 2 1 1 440 #439=(619,256,7,7)f32 #440=(619,256,7,7)f32
torch.eq                 torch.eq_135             1 1 433 441 other=1 $input=433 #433=(1000)i64 #441=(1000)bool
aten::where              pnnx_3622                1 1 441 442 #441=(1000)bool
prim::ListUnpack         pnnx_3623                1 1 442 443 #443=(36)i64
pnnx.Expression          pnnx_expr_323            1 1 443 444 expr=[@0] #443=(36)i64
Tensor.index             Tensor.index_585         2 1 421 444 445 $input=421 $expr=444 #421=(1000,5)f32 #445=(36,5)f32
pnnx.custom_op.torchvision.roi_align pnnx_3630                2 1 277 445 446 arg_2=0.125 arg_3=7 arg_4=7 arg_5=2 arg_6=False $arg_0=277 $arg_1=445 #277=(1,256,100,100)f32 #445=(36,5)f32 #446=(36,256,7,7)f32
aten::to                 pnnx_3634                4 1 446 2 1 1 447 #446=(36,256,7,7)f32 #447=(36,256,7,7)f32
torch.eq                 torch.eq_136             1 1 433 448 other=2 $input=433 #433=(1000)i64 #448=(1000)bool
aten::where              pnnx_3637                1 1 448 449 #448=(1000)bool
prim::ListUnpack         pnnx_3638                1 1 449 450 #450=(298)i64
pnnx.Expression          pnnx_expr_314            1 1 450 451 expr=[@0] #450=(298)i64
Tensor.index             Tensor.index_586         2 1 421 451 452 $input=421 $expr=451 #421=(1000,5)f32 #452=(298,5)f32
pnnx.custom_op.torchvision.roi_align pnnx_3645                2 1 273 452 453 arg_2=0.0625 arg_3=7 arg_4=7 arg_5=2 arg_6=False $arg_0=273 $arg_1=452 #273=(1,256,50,50)f32 #452=(298,5)f32 #453=(298,256,7,7)f32
aten::to                 pnnx_3649                4 1 453 2 1 1 454 #453=(298,256,7,7)f32 #454=(298,256,7,7)f32
torch.eq                 torch.eq_137             1 1 433 455 other=3 $input=433 #433=(1000)i64 #455=(1000)bool
aten::where              pnnx_3652                1 1 455 456 #455=(1000)bool
prim::ListUnpack         pnnx_3653                1 1 456 457 #457=(47)i64
pnnx.Expression          pnnx_expr_305            1 1 457 458 expr=[@0] #457=(47)i64
Tensor.index             Tensor.index_587         2 1 421 458 459 $input=421 $expr=458 #421=(1000,5)f32 #459=(47,5)f32
pnnx.custom_op.torchvision.roi_align pnnx_3660                2 1 269 459 460 arg_2=0.03125 arg_3=7 arg_4=7 arg_5=2 arg_6=False $arg_0=269 $arg_1=459 #269=(1,256,25,25)f32 #459=(47,5)f32 #460=(47,256,7,7)f32
aten::to                 pnnx_3664                4 1 460 2 1 1 461 #460=(47,256,7,7)f32 #461=(47,256,7,7)f32
pnnx.Attribute           pnnx_fold_res.1          0 1 462 @data=(1000,256,7,7)f32 #462=(1000,256,7,7)f32
Tensor.reshape           Tensor.reshape_528       1 1 436 463 shape=(619,1,1,1) $input=436 #436=(619)i64 #463=(619,1,1,1)i64
Tensor.expand            Tensor.expand_172        1 1 463 464 shape=(619,256,7,7) $input=463 #463=(619,1,1,1)i64 #464=(619,256,7,7)i64
pnnx.Expression          pnnx_expr_285            0 1 465 expr=0
aten::scatter            pnnx_3716                4 1 462 465 464 440 466 #462=(1000,256,7,7)f32 #464=(619,256,7,7)i64 #440=(619,256,7,7)f32 #466=(1000,256,7,7)f32
Tensor.reshape           Tensor.reshape_530       1 1 443 467 shape=(36,1,1,1) $input=443 #443=(36)i64 #467=(36,1,1,1)i64
Tensor.expand            Tensor.expand_173        1 1 467 468 shape=(36,256,7,7) $input=467 #467=(36,1,1,1)i64 #468=(36,256,7,7)i64
aten::scatter            pnnx_3747                4 1 466 465 468 447 469 #466=(1000,256,7,7)f32 #468=(36,256,7,7)i64 #447=(36,256,7,7)f32 #469=(1000,256,7,7)f32
Tensor.reshape           Tensor.reshape_532       1 1 450 470 shape=(298,1,1,1) $input=450 #450=(298)i64 #470=(298,1,1,1)i64
Tensor.expand            Tensor.expand_174        1 1 470 471 shape=(298,256,7,7) $input=470 #470=(298,1,1,1)i64 #471=(298,256,7,7)i64
aten::scatter            pnnx_3778                4 1 469 465 471 454 472 #469=(1000,256,7,7)f32 #471=(298,256,7,7)i64 #454=(298,256,7,7)f32 #472=(1000,256,7,7)f32
Tensor.reshape           Tensor.reshape_534       1 1 457 473 shape=(47,1,1,1) $input=457 #457=(47)i64 #473=(47,1,1,1)i64
Tensor.expand            Tensor.expand_175        1 1 473 474 shape=(47,256,7,7) $input=473 #473=(47,1,1,1)i64 #474=(47,256,7,7)i64
aten::scatter            pnnx_3809                4 1 472 465 474 461 475 #472=(1000,256,7,7)f32 #474=(47,256,7,7)i64 #461=(47,256,7,7)f32 #475=(1000,256,7,7)f32
torch.flatten            torch.flatten_538        1 1 475 476 end_dim=-1 start_dim=1 $input=475 #475=(1000,256,7,7)f32 #476=(1000,12544)f32
nn.Linear                m.roi_heads.box_head.fc6 1 1 476 477 bias=True in_features=12544 out_features=1024 @bias=(1024)f32 @weight=(1024,12544)f32 #476=(1000,12544)f32 #477=(1000,1024)f32
F.relu                   F.relu_714               1 1 477 478 $input=477 #477=(1000,1024)f32 #478=(1000,1024)f32
nn.Linear                m.roi_heads.box_head.fc7 1 1 478 479 bias=True in_features=1024 out_features=1024 @bias=(1024)f32 @weight=(1024,1024)f32 #478=(1000,1024)f32 #479=(1000,1024)f32
F.relu                   F.relu_715               1 1 479 480 $input=479 #479=(1000,1024)f32 #480=(1000,1024)f32
torch.flatten            torch.flatten_539        1 1 480 481 end_dim=-1 start_dim=1 $input=480 #480=(1000,1024)f32 #481=(1000,1024)f32
nn.Linear                m.roi_heads.box_predictor.cls_score 1 1 481 482 bias=True in_features=1024 out_features=91 @bias=(91)f32 @weight=(91,1024)f32 #481=(1000,1024)f32 #482=(1000,91)f32
nn.Linear                m.roi_heads.box_predictor.bbox_pred 1 1 481 483 bias=True in_features=1024 out_features=364 @bias=(364)f32 @weight=(364,1024)f32 #481=(1000,1024)f32 #483=(1000,364)f32
aten::to                 pnnx_3840                4 1 418 2 1 1 484 #418=(1000,4)f32 #484=(1000,4)f32
torch.unbind             Tensor.select_643        1 4 484 485 486 487 488 dim=1 #484=(1000,4)f32 #485=(1000)f32 #486=(1000)f32 #487=(1000)f32 #488=(1000)f32
pnnx.Expression          pnnx_expr_234            2 1 487 485 489 expr=sub(@0,@1) #487=(1000)f32 #485=(1000)f32 #489=(1000)f32
pnnx.Expression          pnnx_expr_221            2 1 488 486 490 expr=sub(@0,@1) #488=(1000)f32 #486=(1000)f32 #490=(1000)f32
pnnx.Expression          pnnx_expr_212            2 1 485 489 491 expr=add(@0,mul(@1,0.5)) #485=(1000)f32 #489=(1000)f32 #491=(1000)f32
pnnx.Expression          pnnx_expr_202            2 1 486 490 492 expr=add(@0,mul(@1,0.5)) #486=(1000)f32 #490=(1000)f32 #492=(1000)f32
Tensor.slice             Tensor.slice_690         1 1 483 493 dim=1 end=2147483647 start=0 step=4 $input=483 #483=(1000,364)f32 #493=(1000,91)f32
Tensor.slice             Tensor.slice_692         1 1 483 494 dim=1 end=2147483647 start=1 step=4 $input=483 #483=(1000,364)f32 #494=(1000,91)f32
Tensor.slice             Tensor.slice_694         1 1 483 495 dim=1 end=2147483647 start=2 step=4 $input=483 #483=(1000,364)f32 #495=(1000,91)f32
pnnx.Expression          pnnx_expr_175            1 1 495 496 expr=div(@0,5.0) #495=(1000,91)f32 #496=(1000,91)f32
Tensor.slice             Tensor.slice_696         1 1 483 497 dim=1 end=2147483647 start=3 step=4 $input=483 #483=(1000,364)f32 #497=(1000,91)f32
pnnx.Expression          pnnx_expr_165            1 1 497 498 expr=div(@0,5.0) #497=(1000,91)f32 #498=(1000,91)f32
torch.clamp              torch.clamp_152          1 1 496 499 max=4.13517 min=None $input=496 #496=(1000,91)f32 #499=(1000,91)f32
torch.clamp              torch.clamp_153          1 1 498 500 max=4.13517 min=None $input=498 #498=(1000,91)f32 #500=(1000,91)f32
torch.unsqueeze          torch.unsqueeze_558      1 1 489 501 dim=1 $input=489 #489=(1000)f32 #501=(1000,1)f32
torch.unsqueeze          torch.unsqueeze_559      1 1 491 502 dim=1 $input=491 #491=(1000)f32 #502=(1000,1)f32
pnnx.Expression          pnnx_expr_152            3 1 493 501 502 503 expr=add(mul(div(@0,10.0),@1),@2) #493=(1000,91)f32 #501=(1000,1)f32 #502=(1000,1)f32 #503=(1000,91)f32
torch.unsqueeze          torch.unsqueeze_560      1 1 490 504 dim=1 $input=490 #490=(1000)f32 #504=(1000,1)f32
torch.unsqueeze          torch.unsqueeze_561      1 1 492 505 dim=1 $input=492 #492=(1000)f32 #505=(1000,1)f32
pnnx.Expression          pnnx_expr_141            3 1 494 504 505 506 expr=add(mul(div(@0,10.0),@1),@2) #494=(1000,91)f32 #504=(1000,1)f32 #505=(1000,1)f32 #506=(1000,91)f32
pnnx.Expression          pnnx_expr_126            2 1 500 504 507 expr=mul(0.5,mul(exp(@0),@1)) #500=(1000,91)f32 #504=(1000,1)f32 #507=(1000,91)f32
pnnx.Expression          pnnx_expr_122            2 1 499 501 508 expr=mul(0.5,mul(exp(@0),@1)) #499=(1000,91)f32 #501=(1000,1)f32 #508=(1000,91)f32
pnnx.Expression          pnnx_expr_120            2 1 503 508 509 expr=sub(@0,@1) #503=(1000,91)f32 #508=(1000,91)f32 #509=(1000,91)f32
pnnx.Expression          pnnx_expr_118            2 1 506 507 510 expr=sub(@0,@1) #506=(1000,91)f32 #507=(1000,91)f32 #510=(1000,91)f32
pnnx.Expression          pnnx_expr_116            2 1 503 508 511 expr=add(@0,@1) #503=(1000,91)f32 #508=(1000,91)f32 #511=(1000,91)f32
pnnx.Expression          pnnx_expr_114            2 1 506 507 512 expr=add(@0,@1) #506=(1000,91)f32 #507=(1000,91)f32 #512=(1000,91)f32
torch.stack              torch.stack_239          4 1 509 510 511 512 513 dim=2 #509=(1000,91)f32 #510=(1000,91)f32 #511=(1000,91)f32 #512=(1000,91)f32 #513=(1000,91,4)f32
torch.flatten            torch.flatten_540        1 1 513 514 end_dim=-1 start_dim=1 $input=513 #513=(1000,91,4)f32 #514=(1000,364)f32
Tensor.reshape           Tensor.reshape_500       1 1 514 515 shape=(1000,91,4) $input=514 #514=(1000,364)f32 #515=(1000,91,4)f32
F.softmax                F.softmax_717            1 1 482 516 dim=-1 $input=482 #482=(1000,91)f32 #516=(1000,91)f32
torch.split              torch.split_226          1 1 515 517 dim=0 split_size_or_sections=(1000) $tensor=515 #515=(1000,91,4)f32 #517=(1000,91,4)f32
torch.split              torch.split_227          1 1 516 518 dim=0 split_size_or_sections=(1000) $tensor=516 #516=(1000,91)f32 #518=(1000,91)f32
Tensor.slice             Tensor.slice_703         1 1 517 519 dim=2 end=2147483647 start=0 step=2 $input=517 #517=(1000,91,4)f32 #519=(1000,91,2)f32
Tensor.slice             Tensor.slice_704         1 1 517 520 dim=2 end=2147483647 start=1 step=2 $input=517 #517=(1000,91,4)f32 #520=(1000,91,2)f32
pnnx.Expression          pnnx_expr_89             1 1 519 521 expr=min(max(@0,0.0),800.0) #519=(1000,91,2)f32 #521=(1000,91,2)f32
pnnx.Expression          pnnx_expr_81             1 1 520 522 expr=min(max(@0,0.0),800.0) #520=(1000,91,2)f32 #522=(1000,91,2)f32
torch.stack              torch.stack_240          2 1 521 522 523 dim=3 #521=(1000,91,2)f32 #522=(1000,91,2)f32 #523=(1000,91,2,2)f32
Tensor.reshape           Tensor.reshape_501       1 1 523 524 shape=(1000,91,4) $input=523 #523=(1000,91,2,2)f32 #524=(1000,91,4)f32
pnnx.Attribute           pnnx_fold_2838           0 1 525 @data=(1,91)i64 #525=(1,91)i64
Tensor.expand_as         Tensor.expand_as_177     2 1 525 518 526 $input=525 $other=518 #525=(1,91)i64 #518=(1000,91)f32 #526=(1000,91)i64
Tensor.slice             Tensor.slice_706         1 1 524 527 dim=1 end=2147483647 start=1 step=1 $input=524 #524=(1000,91,4)f32 #527=(1000,90,4)f32
Tensor.slice             Tensor.slice_708         1 1 518 528 dim=1 end=2147483647 start=1 step=1 $input=518 #518=(1000,91)f32 #528=(1000,90)f32
Tensor.slice             Tensor.slice_710         1 1 526 529 dim=1 end=2147483647 start=1 step=1 $input=526 #526=(1000,91)i64 #529=(1000,90)i64
Tensor.reshape           Tensor.reshape_502       1 1 527 530 shape=(90000,4) $input=527 #527=(1000,90,4)f32 #530=(90000,4)f32
Tensor.reshape           Tensor.reshape_503       1 1 528 531 shape=(90000) $input=528 #528=(1000,90)f32 #531=(90000)f32
Tensor.reshape           Tensor.reshape_504       1 1 529 532 shape=(90000) $input=529 #529=(1000,90)i64 #532=(90000)i64
torch.gt                 torch.gt_148             1 1 531 533 other=0.05 $input=531 #531=(90000)f32 #533=(90000)bool
aten::where              pnnx_4149                1 1 533 534 #533=(90000)bool
prim::ListUnpack         pnnx_4150                1 1 534 535 #535=(0)i64
pnnx.Expression          pnnx_expr_47             1 1 535 536 expr=[@0] #535=(0)i64
Tensor.index             Tensor.index_588         2 1 530 536 537 $input=530 $expr=536 #530=(90000,4)f32 #537=(0,4)f32
Tensor.index             Tensor.index_589         2 1 531 536 538 $input=531 $expr=536 #531=(90000)f32 #538=(0)f32
Tensor.index             Tensor.index_590         2 1 532 536 539 $input=532 $expr=536 #532=(90000)i64 #539=(0)i64
torch.unbind             Tensor.select_649        1 4 537 540 541 542 543 dim=1 #537=(0,4)f32 #540=(0)f32 #541=(0)f32 #542=(0)f32 #543=(0)f32
pnnx.Expression          pnnx_expr_39             2 1 542 540 544 expr=sub(@0,@1) #542=(0)f32 #540=(0)f32 #544=(0)f32
pnnx.Expression          pnnx_expr_33             2 1 543 541 545 expr=sub(@0,@1) #543=(0)f32 #541=(0)f32 #545=(0)f32
torch.ge                 torch.ge_146             1 1 544 546 other=0.01 $input=544 #544=(0)f32 #546=(0)bool
torch.ge                 torch.ge_147             1 1 545 547 other=0.01 $input=545 #545=(0)f32 #547=(0)bool
pnnx.Expression          pnnx_expr_31             2 1 546 547 548 expr=and(@0,@1) #546=(0)bool #547=(0)bool #548=(0)bool
aten::where              pnnx_4177                1 1 548 549 #548=(0)bool
prim::ListUnpack         pnnx_4178                1 1 549 550 #550=(0)i64
pnnx.Expression          pnnx_expr_30             1 1 550 551 expr=[@0] #550=(0)i64
Tensor.index             Tensor.index_591         2 1 537 551 552 $input=537 $expr=551 #537=(0,4)f32 #552=(0,4)f32
Tensor.index             Tensor.index_592         2 1 538 551 553 $input=538 $expr=551 #538=(0)f32 #553=(0)f32
Tensor.index             Tensor.index_593         2 1 539 551 554 $input=539 $expr=551 #539=(0)i64 #554=(0)i64
aten::numel              pnnx_4191                1 1 552 555 #552=(0,4)f32
torch.eq                 torch.eq_142             1 1 555 556 other=0 $input=555
prim::If                 pnnx_4193                1 1 556 557 #557=(0)i64
pnnx.Expression          pnnx_expr_23             1 1 557 558 expr=[@0] #557=(0)i64
Tensor.index             Tensor.index_594         2 1 552 558 559 $input=552 $expr=558 #552=(0,4)f32 #559=(0,4)f32
Tensor.index             Tensor.index_595         2 1 553 558 560 $input=553 $expr=558 #553=(0)f32 #560=(0)f32
Tensor.index             Tensor.index_596         2 1 554 558 561 $input=554 $expr=558 #554=(0)i64 #561=(0)i64
torch.unbind             torch.unbind_245         1 4 559 562 563 564 565 dim=1 $input=559 #559=(0,4)f32 #562=(0)f32 #563=(0)f32 #564=(0)f32 #565=(0)f32
pnnx.Expression          pnnx_expr_5              1 1 562 566 expr=mul(@0,1.0) #562=(0)f32 #566=(0)f32
pnnx.Expression          pnnx_expr_4              1 1 564 567 expr=mul(@0,1.0) #564=(0)f32 #567=(0)f32
pnnx.Expression          pnnx_expr_3              1 1 563 568 expr=mul(@0,div(torch.uint8,800)),torch.int8,800 #563=(0)f32 #568=(0)f32
pnnx.Expression          pnnx_expr_2              1 1 565 569 expr=mul(@0,div(torch.uint8,800)),torch.int8,800 #565=(0)f32 #569=(0)f32
torch.stack              torch.stack_241          4 1 566 568 567 569 570 dim=1 #566=(0)f32 #568=(0)f32 #567=(0)f32 #569=(0)f32 #570=(0,4)f32
aten::to                 pnnx_4245                4 1 561 2 1 1 571 #561=(0)i64 #571=(0)f32
prim::TupleConstruct     pnnx_4246                3 1 570 560 571 572 #570=(0,4)f32 #560=(0)f32 #571=(0)f32
pnnx.Output              pnnx_output_0            1 0 572
